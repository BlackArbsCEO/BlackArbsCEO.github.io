{"nbformat_minor": 0, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "version": "3.4.3", "pygments_lexer": "ipython3"}}, "cells": [{"cell_type": "markdown", "source": "# SPDR ETF Composite Sector Valuation [Friday 4.10.2015]\n=============================================================================================\n\n### The following Ipython Notebook examines the Implied Cost of Capital (ICC) method of valuation for purposes of trade/portfolio positioning. The goal is to identify asymmetric investing opportunities due to incongruence between 'recent' historical valuations and forward looking expectations of earnings growth. \n\n### I will attempt to accomplish the goal by first examining composite returns based on ETF category groupings. Then I will compare the historical data vs the forward looking ICC estimate to see if we can find some disagreement. \n\n######**Please note there will be some overlap as some of the groupings include international sector ETF's while other groupings contain regional and/or country ETF's. ", "metadata": {}}, {"cell_type": "markdown", "source": "### First set up the environment, lists and other necessary metadata", "metadata": {}}, {"source": "# ================================================================== #\n# composite returns; vol; risk adjusted returns; correlation matrix, ICC analysis\n\nfrom pprint import pprint as pp\nimport pandas as p\nimport numpy as np\nimport pandas.io.data as web\nfrom pandas.tseries.offsets import *\nimport datetime as dt\nimport math\nimport seaborn as sns\nsns.set_style('white')\n\nimport matplotlib.pyplot as plt\n\nimport plotly.plotly as py\nfrom plotly.graph_objs import *\nimport plotly.tools as tls\n\nimport cufflinks \n\n# ================================================================== #\n\ndate_today = dt.date.today()\nmonth = 'APR-2015'\n\n# ~~~ Market Cap ~~~ #\nBroad_mkts = ['THRK','RSCO'] # Russell 3000, Russell Small Cap Completeness\nLarge_cap  = ['ONEK','SPY','SPYG','SPYV'] # Russell 1000, sp500 (growth, value)\nMid_cap    = ['MDY','MDYG', 'MDYV'] # sp400 mid (growth, value)\nSmall_cap  = ['TWOK','SLY','SLYG','SLYV'] # russ 2K, sp600, (growth, value)\n\n# ~~~ International/Global Equities ~~~ #\nGlobal = [\n        'DGT', #  global dow\n        'BIK', # sp BRIC 40 ETF\n        'GMM', # sp emerging mkts \n        'EWX', # sp emerging mkts small caps\n        'CWI', # msci acwi ex-US\n        'GII', # global infrastructure\n        'GNR', # global natural resources\n        'DWX', # intl dividends\n        'GWL', # sp developed world ex-US \n        'MDD', # intl mid cap (2B-5B USD)\n        'GWX'  # intl small cap (<2B USD)\n        ]\n\nAsia   = ['JPP','JSC','GXC','GMF'] # japan, smallcap japan, china, emg asiapac\nEurope = ['FEZ','GUR','RBL','FEU'] # euro stoxx 50, emg europe, russia, stoxx europe 50\nLatam  = ['GML'] # emg latin america\nAfrica = ['GAF'] # emg mideast/africa\n\n# ~~~ Real Assets ~~~ #\nReal_assets = [ 'RWO', # global real estate\n                'RWX', # intl real estate ex-US\n                'RWR'  # US select REIT\n                ]        \n\n# ~~~ sectors and industries ETF's ~~~ #\nSector = [\n          'XLY','XHB','IPD','XRT',                   # consumer discretionary\n          'XLP','IPS',                               # consumer staples\n          'XLE','IPW','XES','XOP',                   # energy\n          'XLF','KBE','KCE','KIE','IPF','KRE',       # financials\n          'XLV','XBI','XHE','XHS','IRY','XPH',       # healthcare\n          'XLI','XAR','IPN','XTN',                   # industrial\n          'XLB','IRV','XME',                         # materials\n          'XLK','MTK','IPK','XSD','XSW',             # technology\n          'IST','XTL',                               # telecom\n          'IPU','XLU'                                # utilities\n          ]\n   \nstock_list = [Broad_mkts, Large_cap, Mid_cap, Small_cap, Global, Asia, Europe, Latam, Africa, Real_assets, Sector]\n\n# ~~~ Category structure ~~~ #\ncat = {'Broad_Market'          :['THRK','RSCO'],\n       'Large_Cap'             :['ONEK','SPY','SPYG','SPYV'],\n       'Mid_Cap'               :['MDY','MDYG', 'MDYV'], \n       'Small_Cap'             :['TWOK','SLY','SLYG','SLYV'],\n       'Global_Equity'         :['DGT','BIK','GMM','EWX','CWI','GII','GNR','DWX','GWL','MDD','GWX'],\n       'AsiaPac_Equity'        :['JPP','JSC','GXC','GMF'],\n       'Europe_Equity'         :['FEZ','GUR','RBL','FEU'],\n       'Latam_MidEast_Africa'  :['GML','GAF'],\n       'Real_Estate'           :['RWO','RWX','RWR'],\n       'Consumer_Discretionary':['XLY','XHB','IPD','XRT'],\n       'Consumer_Staples'      :['XLP','IPS'],                         \n       'Energy'                :['XLE','IPW','XES','XOP'],                   \n       'Financials'            :['XLF','KBE','KCE','KIE','IPF','KRE'],\n       'Healthcare'            :['XLV','XBI','XHE','XHS','IRY','XPH'],\n       'Industrial'            :['XLI','XAR','IPN','XTN'],\n       'Materials'             :['XLB','IRV','XME'],\n       'Technology'            :['XLK','MTK','IPK','XSD','XSW'],\n       'Telecom'               :['IST','XTL'],                            \n       'Utilities'             :['IPU','XLU']\n        }    \n\nfilepath   = r'C:\\Users\\Owner\\Documents\\Visual Studio 2013\\Projects\\iVC_Reporting_Engine\\PythonApplication2\\\\'", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": true}, "outputs": []}, {"cell_type": "markdown", "source": "### Now I define a convenience function to grab stock prices. ", "metadata": {}}, {"source": "# ================================================================== #\n# get prices\ndef get_px(stock, start, end):\n    '''\n    Function to call Pandas' Yahoo Finance API to get daily stock prices.\n    \n    Parameters:\n    ==========\n    stock = type('str'); stock symbol \n    start = 3 business days before today; datetime date_today object offset by pandas.DateOffset method \n    end   = today; datetime date_today object\n\n    Returns:\n    ========\n    time series = Pandas.Series object corresponding to stock symbol, and start/end dates\n    **Note that if price column is not specified the function will return a Pandas.DataFrame object\n    '''      \n    try:\n        return web.DataReader(stock, 'yahoo', start, end)['Adj Close']\n    except Exception as e:\n        print( 'something is fucking up' )\n\npx = p.DataFrame()\nfor category in stock_list:\n    for stock in category:\n        px[stock] = get_px( stock, date_today - 252 * BDay(), date_today )", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": true}, "outputs": []}, {"cell_type": "markdown", "source": "### Calculate the log returns for our ETF's then construct the dataframe with proper multi index for grouping", "metadata": {}}, {"source": "# ================================================================== #\n# construct dataframe and proper multi index\nlog_rets = np.log( px / px.shift(1) ).dropna()\n\nlrets = log_rets.T.copy()\nlrets.index.name = 'ETF'\nlrets['Category'] = p.Series()\n\nfor cat_key, etf_val in cat.items():\n    for val in etf_val:\n        if val in lrets.index:\n            idx_loc = lrets.index.get_loc(val)\n            lrets.ix[idx_loc,'Category'] = cat_key\n        else:\n            pass\n\nlrets.set_index('Category', append=True, inplace=True)\nlrets = lrets.swaplevel('ETF','Category').sortlevel('Category')\nlrets.head()", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### Calculate the cumulative returns of each ETF", "metadata": {}}, {"source": "# ================================================================== #\n# cumulative returns of ETF's\ncum_rets = lrets.groupby(level='Category').cumsum(axis=1)\ncum_rets.head()", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### Calculate a composite cumulative return based on the ETF categories. Each ETF is given equal weighting within its own category. I plot the result.", "metadata": {}}, {"source": "# ================================================================== #\n# composite groupings of cumulative ETF returns (equally weighted intra-category mean returns)\ncomposite_rets = p.DataFrame()\nfor label in cat.keys():\n    composite_rets[label] = cum_rets.ix[label].mean(axis=0) # equal weighted mean\n    \ncomp_rets = np.round(composite_rets.copy(),4) # rounding\n\n# to demonstrate the enhanced interactivity of Ploty charts first plot matplotlib\n%matplotlib inline\n\nsize=(14,12)\ncomposite_rets.plot(figsize=size)\nplt.show()", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### The basic plot is ok but how much can you really interpret from the previous time series given that there are 19 different categories? Probably not much. Good thing I have a solution. \n\n### I use a combination of `Plotly's API` and the `Cufflinks` module to create interactive charts. Plotly's charts allow for: zoom, pan, zoom-in, zoom-out, autoscale in addition to the awesome interactive data display. \n\n### Below I create a convenince function to construct the layout I want to pass to the `df.iplot` method. ", "metadata": {}}, {"source": "# ~~~~~ plot code ~~~~~ \n# function to create Plotly 'Layout' object\n\ndef create_layout( main_title, x_title, y_title ):\n    '''\n    Function to create custom Plotly layout object to pass to Cufflinks df.iplot() method\n    \n    Parameters:\n    ==========\n    \n    main_title = type('str')\n    x_title    = type('str')\n    y_title    = type('str')\n\n    Returns:\n    ========\n    plotly_layout = Plotly Layout object basically constructed using a JSON or Dict structure    \n    '''    \n    plotly_layout = Layout(\n        # ~~~~ construct X axis\n        xaxis=XAxis(\n            title= x_title,\n            titlefont=Font(\n                family='oxygen, sans-serif',\n                size=18,\n                color='grey'\n            ),\n            showticklabels=True,\n            tickangle=-30,\n            tickfont=Font(\n                family='oxygen, sans-serif',\n                size=11,\n                color='black'\n            ),\n            exponentformat='e',\n            showexponent='All'\n        ),\n        # ~~~~ construct Y axis\n        yaxis=YAxis(\n            title= y_title,\n            titlefont=Font(\n                family='oxygen, sans-serif',\n                size=18,\n                color='grey'\n            ),\n            showticklabels=True,\n            tickangle=0,\n            tickfont=Font(\n                family='oxygen, sans-serif',\n                size=11,\n                color='black'\n            ),\n            exponentformat='e',\n            showexponent='All'),\n        # ~~~~ construct figure size\n            autosize=False,\n            title= main_title,\n            width=1000,\n            height=500,\n            margin=Margin(\n            l=60,\n            r=30,\n            b=70,\n            t=30,\n            pad=5\n        )\n    )\n    return plotly_layout", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": true}, "outputs": []}, {"cell_type": "markdown", "source": "### See the resulting plot below", "metadata": {}}, {"source": "# test the function\ntitle = 'Cumulative Log Returns of Composite ETF Sectors [1 Year]'\nx_label = 'Date'\ny_label = 'Return'\n\ncustom_layout_1 = create_layout( title, x_label, y_label )\ncomp_rets.iplot(theme='white',filename='Tutorial White', layout=custom_layout_1, world_readable=True)", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### Calculate the ETF category standard deviation of returns. Then construct a moving average of the composite sigmas and plot the data", "metadata": {}}, {"source": "# ================================================================== #\n# composite rolling std\n\nsigmas = lrets.groupby(level='Category').std() # equal weighted std\n\ncomposite_sigs = p.DataFrame()\nfor label in cat.keys():\n    composite_sigs[label] = sigmas.ix[label] \n\nrsigs = p.rolling_mean( composite_sigs, window=60 ).dropna()*math.sqrt(60)\n\n# ~~~~~ plot code\ntitle = '60 day Moving Average of Standard Deviation'\ny_label = '$\\sigma$'\nx_label = 'Date'\ncustom_layout_2 = create_layout( title, x_label, y_label )\nrsigs.iplot(theme='white',filename='Tutorial White', layout=custom_layout_2, world_readable=True)", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### Now I calculate a risk adjusted return metric. For simplicity I assume there is no benchmark ETF return. I use the formula: \n\n## $R_{risk} = \\mu/\\sigma$\n\n### I plot the result below", "metadata": {}}, {"source": "# ================================================================== #\n# composite rolling risk adjusted returns\n\nmean_rets = lrets.groupby(level='Category').mean() # equal weighted mean\n#risk_rets = (mean_rets-lrets.loc['Global_Equity','DGT'])/sigmas\n#risk_rets = mean_rets/sigmas\n\ncomposite_risk_rets = p.DataFrame()\nfor label in cat.keys():\n    composite_risk_rets[label] = mean_rets.ix[label] \n\nrs = p.rolling_mean( composite_risk_rets, window=60 ).dropna() \nrisk_rets = rs/rsigs\n\n# ~~~~~ plot code\ntitle = r'60 day Moving Average of Composite Risk-Adjusted Returns'\nx_label = 'Date'\ny_label = 'Risk-Adjusted Return'\n\ncustom_layout_3 = create_layout( title, x_label, y_label )\nrisk_rets.iplot(theme='white', filename='Tutorial White', layout=custom_layout_3, world_readable=True)", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### For portfolio positioning/rebalancing we need to see the correlations of the return data. To do that I construct a composite correlation matrix and plot the results. ", "metadata": {}}, {"source": "# ================================================================== #\n# correlation matrix of composite ETF groups' risk adjusted returns\n\ncor = risk_rets.corr()\n\n# ~~ plot code\nf, ax = plt.subplots(figsize=(14, 12))\n\ncmap = sns.diverging_palette(h_neg=12, h_pos=144, s=91, l=44, sep=29, n=12, center='light',as_cmap=True)\nsns.corrplot(cor, annot=False, sig_stars=False, diag_names=False, cmap=cmap, ax=ax)\nax.set_title('Composite ETF Groups Correlation Matrix', fontsize=18)\n\nfor label in (ax.get_xticklabels() + ax.get_yticklabels()):\n    label.set_fontsize(13)\n\nf.tight_layout()", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "scrolled": false, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### Now that we have our historical return analysis I need to import my `ICC` estimates. \n------------------\n### A quick review of the `ICC` model for the uninitiated:\n\n### The `ICC` model is really an `internal rate of return (IRR) ` metric that uses the `Residual Income Model` of valuation. I present the basic formula below:\n\n## ${Price}_{0} = {BVPS}_{0} +$ $\\sum\\limits_{t=1}^{T}$ $\\frac {RI_{t}}{(1 + r)^t} +$ $\\frac {{Terminal} {Value}}{(1 + r)^t}$\n### where: \n#### $RI_{t} =$ ${EPS}_{t} - $$({r} * {BVPS}_{t-1})$\n#### ${Terminal}$ ${Value} =$ ${EPS}_{t} * (1 + g) / (r - g)$\n#### $g =$ $estimated$ $long$ $term$ $growth$ $rate$\n#### $r = $$cost$ $of$ $equity$", "metadata": {}}, {"cell_type": "markdown", "source": "### By solving for `r` aka the `cost of equity` we can estimate a market implied cost of equity or `ICC` for our purposes. This methodology allows us to solve for the market expected rate of growth over the estimated period that equates the future value of the stock to today's prices. The beauty of the metric is that it uses analysts' consensus `EPS` forecast to \"solve backwards\" to today's prices giving us a view into institutional investors' market expectations. \n\n### Before some smart ass chimes in to say equity analysts' estimates are \"bullshit\" or consistently \"wrong\" I would respond you're missing the point. My goal in calculating this metric is to take a snapshot of current market expectations (psychology) of the future. Analyst accuracy is largely irrelevant. \n\n### The simple reason being that the market trades on expectations of future events. If you can understand what your opponents' expectations are you have an advantage. \n\n### A simple example: If the calculation reveals the market expectations for future earnings growth of a sector like `Utilities (XLU)` is relatively low compared to its historical stock performance, an astute investor would interpret the signal as `\"the sector is ripe for correction and likely to surprise to the downside\"`. This could be because earnings estimates are declining or the stock price has already \"priced in\" future growth. The point being that it would be unwise to be a buyer at these hypothetical levels as the data hints at a likely \"top\" in value barring any opposing identifiable catalyst. ", "metadata": {}}, {"source": "# ================================================================== #\n# import ICC estimates\nframe = p.read_csv( filepath+'Spdr_ICC_est_{}.csv'.format(date_today) , index_col=0 ).dropna()\n# ================================================================== #\n# group ICC data by category\nf        = frame.copy()\ngrp      = f.groupby('Category')\ngrp_mean = grp.mean().sort('ETF_ICC_est', ascending=False)", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": true}, "outputs": []}, {"source": "grp_mean = np.round( grp_mean['ETF_ICC_est'], 4 )\ngrp_mean", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### I'm a fan of z_scoring data like this so I can get a better sense of the extreme values ", "metadata": {}}, {"source": "def z_score(df):\n    return ( df - df.mean() ) / df.std()\n\n#z_grp = (grp_mean - grp_mean.mean()) / grp_mean.std()\nz_grp = z_score(grp_mean)\n\nplt.figure(figsize=size);\nz_grp.plot('barh')\nplt.axvline(0, color='k')\nplt.title('Z Score of ICC Estimates by Category', fontsize=20)\nplt.xlabel('$\\sigma$', fontsize=20)\nplt.ylabel('Category', fontsize=14)\nplt.tick_params(axis='both', which='major', labelsize=14)\n", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### For comparative purposes I use the an average of the last 30 day's risk adjusted returns by ETF category. I then z score the data and plot it", "metadata": {}}, {"source": "# last 30 days average category risk adjusted returns\ndate_mask = date_today - 30 * BDay()\nl_30 = risk_rets.ix[date_mask:].mean().order(ascending=False) \nl_30", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"source": "# z scored and plotted\nz_l_30 = z_score(l_30)\n\nplt.figure(figsize=size);\nz_l_30.plot('barh',color='r')\nplt.axvline(0, color='k')\nplt.title('Z Score of Average Risk Adjusted Returns for last 30 days', fontsize=20)\nplt.xlabel('$\\sigma$', fontsize=20)\nplt.ylabel('Category', fontsize=14)\nplt.tick_params(axis='both', which='major', labelsize=14)", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### Finally I construct a dataframe comprised of both series of z scored data and plot them for comparison purposes. Again we are looking for 'significant' discrepencies between historical return data and market implied EPS growth expectations. Let's plot the results.", "metadata": {}}, {"source": "z_data = p.DataFrame()\nz_data['z_ICC est'] = z_grp\nz_data['z_rets_adj'] = z_l_30\n# z_data.head()\n\nplt.figure()\nz_data.plot(kind='barh',figsize=size)\nplt.axvline(0, color='k')\nplt.title('Z Scores Comparison', fontsize=20)\nplt.xlabel('$\\sigma$', fontsize=20)\nplt.ylabel('Category', fontsize=14)\nplt.tick_params(axis='both', which='major', labelsize=14)", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": false}, "outputs": []}, {"cell_type": "markdown", "source": "### There are some interesting results here. \n\n### Potential Long Positions:\n================================\n### - Europe_Equity is intriguing. Already off to a relatively strong start on the year, they are actively devaluing the `EURO` and effectively using the `ECB` as put on the bond market. If the `ECB` can replicate what the `Fed` did for U.S. risk assets European equity markets could be setting up for a long term bull run. At the very least we government tailwinds behind any long position. \n### - Financials. Comparatively they have the highest implied growth rate moving forward. Generally I'm not a fan of the sector due to the complexity and diversity of the firms within it but broadly speaking, financials have room to run if the market believes the `Fed` will ever raises rates. It is assumed a rise in rates would improve the net interest margin for domestic banking institutions.\n\n### Potential Short Positions:\n================================\n### - Mid Caps look ripe for correction if i may quote my previous example. Market implied growth is in the bottom half of z scored data but a look at the risk adjusted returns relative to the other sectors shows massive outperformance. To me that would indicate an overbought situation with asymmetric risk to the downside. At the very least the mid cap grouping warrants more investigation for overvalued ETF's and individual stocks. If price action confirms my bear bias I would look to get short. \n\n-------", "metadata": {}}, {"cell_type": "markdown", "source": "### I conclude this analysis with the disclaimer that these calculations are presented `\"as is\"` and the data was aggregated from several sources. I recommend doing your own due diligence before taking any investment action and to stay within your personal risk/return objectives. \n\n### I will be making refinements as necessary as I personally use this methodology to get a macro valuation perspective. \n\n### For comments, questions, and feedback contact me at `bcr@blackarbs.com`", "metadata": {}}, {"source": "", "cell_type": "code", "execution_count": null, "metadata": {"trusted": true, "collapsed": true}, "outputs": []}]}